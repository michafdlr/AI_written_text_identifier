{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "817e908e",
   "metadata": {},
   "source": [
    "# Goal of the final project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79bb052c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f52de02",
   "metadata": {},
   "source": [
    "In recent years, we have witnessed a remarkable revolution in the field of artificial intelligence (AI). The rapid advancements in machine learning and neural networks have unlocked unprecedented capabilities, enabling AI systems to perform complex tasks and mimic human intelligence with remarkable accuracy. This revolution has had a profound impact on various aspects of our lives, including communication, entertainment, and information dissemination.\n",
    "\n",
    "**One particular area of concern that has emerged in the wake of the AI revolution is the generation of synthetic content.** AI-powered systems are now capable of producing highly realistic and convincing text, images, videos, and audio that can be indistinguishable from those created by humans. While this technological advancement has brought about exciting possibilities, it has also raised important questions and challenges regarding the authenticity and reliability of the content we encounter.\n",
    "\n",
    "The ability to identify content that was produced by AI has become increasingly crucial in today's digital landscape. The sheer scale and speed at which AI-generated content can be disseminated pose significant risks to individuals, organizations, and society as a whole. Misinformation, fake news, and fraudulent activities are some of the detrimental consequences that can arise if AI-generated content goes unchecked and unverified.\n",
    "\n",
    "Moreover, the rise of deepfakes, which are AI-generated media that superimpose one person's likeness onto another's body, has heightened concerns about the potential for malicious misuse of AI technology. Deepfakes have the potential to deceive and manipulate individuals, erode trust in visual evidence, and create social and political chaos. In a world where anyone with access to AI algorithms can generate highly realistic but fabricated content, the need to distinguish between what is genuine and what is artificially created is paramount.\n",
    "\n",
    "Identifying AI-generated content is also essential to preserve the integrity of intellectual property and protect creative works. AI algorithms are capable of creating original music compositions, artwork, and even written works. Ensuring proper attribution and copyright protection becomes more challenging when AI systems can mimic the style and creativity of human creators. Differentiating between human-authored and AI-generated content helps maintain the ethical and legal boundaries surrounding intellectual property rights.\n",
    "\n",
    "Additionally, understanding the origin and nature of AI-generated content is crucial for informed decision-making and public discourse. Transparently labeling AI-generated content allows individuals to assess the credibility and bias of the information they consume. It empowers users to make more informed judgments, fosters critical thinking, and safeguards against the manipulation of public opinion.\n",
    "\n",
    "In conclusion, as the AI revolution continues to shape our digital landscape, the ability to identify content produced by AI has become vital. Doing so helps combat the spread of misinformation, protect intellectual property rights, and preserve the integrity of public discourse. By developing robust tools, guidelines, and awareness surrounding AI-generated content, we can navigate this new era of technological advancements more responsibly and confidently.\n",
    "\n",
    "-- *Generated Introduction by ChatGPT from 30th May 2023*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5154952",
   "metadata": {},
   "source": [
    "## What we will do"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efb0d31d",
   "metadata": {},
   "source": [
    "We will try to develop a classifier that is capable of identifying AI written content. This notebook should give a quick starting point and an overview of what we can do for our project. We might also create further AI generated text by more advanced models like GPT 3.x or later and other similar models. For this purpose we can get an API key from OpenAI to get text data from newer GPT models by writing a python script to quickly access AI text. We should talk about it next week..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "951dc64b",
   "metadata": {},
   "source": [
    "# Create a baseline model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96d419f1",
   "metadata": {},
   "source": [
    "## Import datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2331b75",
   "metadata": {},
   "source": [
    "Download whole data from kaggle (other options possible -> github repo from OpenAI; takes longer and sometimes stops downloading) into a local folder at first; maybe we move it to the cloud later.\n",
    "https://www.kaggle.com/datasets/abhishek/gpt2-output-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a20ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA_texts.ipynb          Prep_final_project.ipynb RoBERTa_Michael_F.ipynb\n"
     ]
    }
   ],
   "source": [
    "# your folder should look like this\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf4615f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large-762M-k40.test.csv   medium-345M.test.csv      webtext.test.csv\r\n",
      "large-762M-k40.train.csv  medium-345M.train.csv     webtext.train.csv\r\n",
      "large-762M-k40.valid.csv  medium-345M.valid.csv     webtext.valid.csv\r\n",
      "large-762M.test.csv       small-117M-k40.test.csv   xl-1542M-k40.test.csv\r\n",
      "large-762M.train.csv      small-117M-k40.train.csv  xl-1542M-k40.train.csv\r\n",
      "large-762M.valid.csv      small-117M-k40.valid.csv  xl-1542M-k40.valid.csv\r\n",
      "medium-345M-k40.test.csv  small-117M.test.csv       xl-1542M.test.csv\r\n",
      "medium-345M-k40.train.csv small-117M.train.csv      xl-1542M.train.csv\r\n",
      "medium-345M-k40.valid.csv small-117M.valid.csv      xl-1542M.valid.csv\r\n"
     ]
    }
   ],
   "source": [
    "! cd data && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c92aab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fbc1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quentinbragard/code/AI_written_text_identifier/notebooks\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8815712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you select to correct path to your files\n",
    "path_data = Path('/Users/quentinbragard/code/AI_written_text_identifier/raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9f726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data depending on size; loads human and AI written text\n",
    "def load_data(source: str=\"xl-1542M\", \n",
    "              truncation: bool=True,\n",
    "              n_rows: int=500_000) -> dict[pd.DataFrame]:\n",
    "    '''Load the data in dictionary of pandas Dataframes.\n",
    "    ---\n",
    "    source: specifies the outputs of a GPT-2 model\n",
    "    \n",
    "    ---\n",
    "    truncation: specifies if Top-K 40 truncation data is used\n",
    "    \n",
    "    ---\n",
    "    n_rows: specifies the fraction of data loaded. Smaller values for testing the code.'''\n",
    "    final_data={}\n",
    "    for split in [\"train\", \"valid\", \"test\"]:\n",
    "        data={}\n",
    "        if truncation:\n",
    "            file_path = path_data / f\"{source}-k40.{split}.csv\"\n",
    "        else:    \n",
    "            file_path = path_data / f\"{source}.{split}.csv\"\n",
    "        data['fake'] = pd.read_csv(file_path, usecols=[\"text\"], nrows=n_rows//2) # nrows to have balanced dataset\n",
    "        data['fake'][\"AI\"] = 1 # AI written\n",
    "        \n",
    "        file_path = path_data / f\"webtext.{split}.csv\"\n",
    "        data['true'] = []\n",
    "        data['true'] = pd.read_csv(file_path, usecols=[\"text\"], nrows=n_rows//2) # nrows to have balanced dataset\n",
    "        data['true'][\"AI\"] = 0 # not AI written\n",
    "        \n",
    "        final_data[split] = pd.concat([data[\"true\"], data[\"fake\"]])\n",
    "        \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d711b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/quentinbragard/code/AI_written_text_identifier/raw_data/xl-1542M-k40.train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500_000\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m data_val \u001b[38;5;241m=\u001b[39m load_data(n_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10_000\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m data_test \u001b[38;5;241m=\u001b[39m load_data(n_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10_000\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(source, truncation, n_rows)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[1;32m     20\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m path_data \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfake\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_rows\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# nrows to have balanced dataset\u001b[39;00m\n\u001b[1;32m     22\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfake\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# AI written\u001b[39;00m\n\u001b[1;32m     24\u001b[0m file_path \u001b[38;5;241m=\u001b[39m path_data \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwebtext.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/AI_written_text_identifier/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/AI_written_text_identifier/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/AI_written_text_identifier/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/AI_written_text_identifier/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/AI_written_text_identifier/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/quentinbragard/code/AI_written_text_identifier/raw_data/xl-1542M-k40.train.csv'"
     ]
    }
   ],
   "source": [
    "data_train = load_data(n_rows=500_000)[\"train\"].reset_index(drop=True)\n",
    "data_val = load_data(n_rows=10_000)[\"valid\"].reset_index(drop=True)\n",
    "data_test = load_data(n_rows=10_000)[\"test\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bfb3141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>AI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These girlfriends deserves a special mention f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeSean McCoy going through warmups with first ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom Curran has been called up to England's Ash...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We'll have turkey on the table Thursday but, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 1945 Sinkings of the Cap Arcona and the Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>There are a lot of things that I don't like ab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>A year after an unprecedented public outcry ag...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>Battles Between the English and the Scots\\n\\nT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>Kurt Rambis is the new head coach of the Knick...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>\"To be a leader... you need to do this or that...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  AI\n",
       "0       These girlfriends deserves a special mention f...   0\n",
       "1       LeSean McCoy going through warmups with first ...   0\n",
       "2       Tom Curran has been called up to England's Ash...   0\n",
       "3       We'll have turkey on the table Thursday but, a...   0\n",
       "4       The 1945 Sinkings of the Cap Arcona and the Th...   0\n",
       "...                                                   ...  ..\n",
       "499995  There are a lot of things that I don't like ab...   1\n",
       "499996  A year after an unprecedented public outcry ag...   1\n",
       "499997  Battles Between the English and the Scots\\n\\nT...   1\n",
       "499998  Kurt Rambis is the new head coach of the Knick...   1\n",
       "499999  \"To be a leader... you need to do this or that...   1\n",
       "\n",
       "[500000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6961cc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    500000 non-null  object\n",
      " 1   AI      500000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 7.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdd2fe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    10000 non-null  object\n",
      " 1   AI      10000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_val.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c5ef5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    10000 non-null  object\n",
      " 1   AI      10000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a96ed116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.5\n",
      "1    0.5\n",
      "Name: AI, dtype: float64\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: AI, dtype: float64\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: AI, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data_train.AI.value_counts(normalize=True))\n",
    "\n",
    "print(data_val.AI.value_counts(normalize=True))\n",
    "\n",
    "print(data_test.AI.value_counts(normalize=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb9b8855",
   "metadata": {},
   "source": [
    "Data is balanced 👍🏽"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce04e573",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84508502",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fdff4f4",
   "metadata": {},
   "source": [
    "Cleaning might not be a good idea for our classification task as removing stopwords, and lowercase everything might worsen performance of a model. We might try different options later. In this https://github.com/openai/gpt-2-output-dataset/blob/master/baseline.py repository (it is from developers at OpenAI) they did not clean the data at all. I also tried it out. Result: with cleaning like in the modules on NLP the accuracy-score is way worse then without cleaning! So we shouldn't clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4ffface",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df71cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text: str) -> str:\n",
    "    text = ''.join([l for l in text if not l in string.punctuation])\n",
    "    return text\n",
    "\n",
    "def lower(text: str) -> str:\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text) -> str:\n",
    "    text = ''.join([char for char in text if not char.isdigit()])\n",
    "    return text\n",
    "\n",
    "def tokenize(text: str) -> list:\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_stopwords(text: list) -> list:\n",
    "    stop_words = set(stopwords.words('english')) # we should be careful, there are also other lang present\n",
    "    tokens_clean = [word for word in text if not word in stop_words]\n",
    "    return tokens_clean\n",
    "\n",
    "def lemmatize(text: list) -> str:\n",
    "    for pos in [\"v\", \"n\", \"a\", \"r\", \"s\"]:\n",
    "        text = [WordNetLemmatizer().lemmatize(word, pos=pos) for word in text]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def clean(text: str, remove_stopword: bool=False, lemmatize: bool=False) -> str:\n",
    "    text = remove_numbers(lower(remove_punctuation(text)))\n",
    "    if remove_stopword or lemmatize:\n",
    "        text=tokenize(text)\n",
    "    if remove_stopword:\n",
    "        text = remove_stopwords(text)\n",
    "    if lemmatize:\n",
    "        text = lemmatize(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d77f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(clean(small_train.loc[0, \"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfcf8149",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_clean = data_train\n",
    "data_val_clean = data_val\n",
    "data_test_clean = data_test\n",
    "#for df in [data_train_clean, data_val_clean, data_test_clean]:\n",
    "#    df.text = df.text.apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ee89fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if ram to small\n",
    "#batch_size= 200\n",
    "#steps = round(small_train_short.shape[0]/batch_size)\n",
    "#small_short_clean = small_train_short\n",
    "#for step in range(steps):\n",
    "#    small_short_clean.iloc[step*batch_size: step*batch_size+batch_size, 0] = small_train_short.iloc[step*batch_size: step*batch_size+batch_size, 0].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f872c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data_val_clean.loc[0,\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c377f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train_clean.AI.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "476a2aac",
   "metadata": {},
   "source": [
    "### Vectorizing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ba1739e",
   "metadata": {},
   "source": [
    "Vectorization is an important part. For the baseline model we could use TfIdfvectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c13d75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, PredefinedSplit\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from scipy import sparse #to handle sparse matrices from the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "911aa1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train_clean[[\"text\"]]\n",
    "X_val = data_val_clean[[\"text\"]]\n",
    "X_test = data_test_clean[[\"text\"]]\n",
    "y_train = data_train_clean[\"AI\"]\n",
    "y_val = data_val_clean[\"AI\"]\n",
    "y_test = data_test_clean[\"AI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e97fd307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500000, 1), (500000,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d687bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>These girlfriends deserves a special mention f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LeSean McCoy going through warmups with first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tom Curran has been called up to England's Ash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We'll have turkey on the table Thursday but, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 1945 Sinkings of the Cap Arcona and the Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>There are a lot of things that I don't like ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>A year after an unprecedented public outcry ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>Battles Between the English and the Scots\\n\\nT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>Kurt Rambis is the new head coach of the Knick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>\"To be a leader... you need to do this or that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0       These girlfriends deserves a special mention f...\n",
       "1       LeSean McCoy going through warmups with first ...\n",
       "2       Tom Curran has been called up to England's Ash...\n",
       "3       We'll have turkey on the table Thursday but, a...\n",
       "4       The 1945 Sinkings of the Cap Arcona and the Th...\n",
       "...                                                   ...\n",
       "499995  There are a lot of things that I don't like ab...\n",
       "499996  A year after an unprecedented public outcry ag...\n",
       "499997  Battles Between the English and the Scots\\n\\nT...\n",
       "499998  Kurt Rambis is the new head coach of the Knick...\n",
       "499999  \"To be a leader... you need to do this or that...\n",
       "\n",
       "[500000 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0376cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), \n",
    "                             max_features=2**18, \n",
    "                             min_df=100)\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(X_train.text) # this takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c249162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_vec = vectorizer.transform(X_val.text)\n",
    "X_test_vec = vectorizer.transform(X_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa2e1d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<500000x234672 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 230050038 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec #this gives a sparse matrix -> we should let it like this because of memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d360fca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_search = sparse.vstack([X_train_vec, X_val_vec]) # to feed in Gridsearch\n",
    "y_search = np.hstack((y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "575f94b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make sure training data is not used for validation\n",
    "split = PredefinedSplit([-1]*X_train.shape[0]+[0]*X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5f5dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'C': [2**k for k in range(3,6)],\n",
    "    #'solver': ['liblinear', 'sag']\n",
    "}\n",
    "\n",
    "model = LogisticRegression(solver=\"liblinear\", max_iter=1000)\n",
    "search = GridSearchCV(model, \n",
    "                     params,\n",
    "                     cv=split,\n",
    "                     n_jobs=-1,\n",
    "                      verbose = 3,\n",
    "                      refit=False,\n",
    "                     scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a28ed760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 3 candidates, totalling 3 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             estimator=LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;),\n",
       "             n_jobs=-1, param_grid={&#x27;C&#x27;: [8, 16, 32]}, refit=False,\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             estimator=LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;),\n",
       "             n_jobs=-1, param_grid={&#x27;C&#x27;: [8, 16, 32]}, refit=False,\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ...,  0,  0])),\n",
       "             estimator=LogisticRegression(max_iter=1000, solver='liblinear'),\n",
       "             n_jobs=-1, param_grid={'C': [8, 16, 32]}, refit=False,\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_search, y_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f80e890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9234"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd77e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = model.set_params(**search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4730aba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=8, max_iter=1000, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=8, max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=8, max_iter=1000, solver='liblinear')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7744fdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=8, max_iter=1000, solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=8, max_iter=1000, solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=8, max_iter=1000, solver='liblinear')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7387cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_score = baseline.score(X_val_vec, y_val)\n",
    "test_score = baseline.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34d733f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9234"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/1] END ...............................C=8;, score=0.923 total time= 3.1min\n",
      "[CV 1/1] END ..............................C=16;, score=0.921 total time= 3.4min\n",
      "[CV 1/1] END ..............................C=32;, score=0.921 total time= 3.8min\n"
     ]
    }
   ],
   "source": [
    "test_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9afc77d8",
   "metadata": {},
   "source": [
    "The baseline model already gives a pretty good accuracy score on which we can iterate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8faff7a6",
   "metadata": {},
   "source": [
    "## Create a better model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9829ad5",
   "metadata": {},
   "source": [
    "Possible ways to improve our baseline-model:\n",
    "- different tokenizers (tiktoken from OpenAI, LlamaTokenizer from huggingface transformers, Roberta tokenizer from huggingface transformer https://huggingface.co/docs/transformers/index)\n",
    "- using word-embeddings instead of tokenizer (Word2Vec and others)\n",
    "- different model from classical ML (SVM Classifier -> takes much memory and takes huge time amounts (was running it wit 10% of the data and each fit took about 245 min!) -> try SGDClassifier https://scikit-learn.org/0.15/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier; Ensemble methods)\n",
    "- DL model (transfer learning with some base model like https://www.tensorflow.org/text/tutorials/classify_text_with_bert at or huggingface transformers https://huggingface.co/roberta-base-openai-detector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2793b0f7",
   "metadata": {},
   "source": [
    "### Try SGDClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c7ab5db",
   "metadata": {},
   "source": [
    "Depending on the loss specified in the SGDClassifier different SGD models are available. Using SGDClassifier allows us to use batches which is needed as the size of our data is huge and training more complicated models will result in having not enough RAM (unless you have a +5k € computer with RAM > 16 GB or when using a VM which we might have to do later to have enough power). For this procedure the most 'difficult' part is to find good parameters of the classifier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ec04a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0,1] # this needs to be specified for the partial_fit method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f805baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an iterator to train the SGDClassifier on batches\n",
    "chunksize=4000\n",
    "data_train_shuffle = data_train_clean.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "X_train_shuffle = data_train_shuffle[[\"text\"]]\n",
    "y_train_shuffle = data_train_shuffle[\"AI\"]\n",
    "\n",
    "def chunk_iterator(chunksize, size_train):\n",
    "    index_start = 0\n",
    "    while index_start < size_train:\n",
    "        X_chunk = X_train_shuffle.iloc[index_start: index_start+chunksize]\n",
    "        X_chunk_vec = vectorizer.transform(X_chunk.text)\n",
    "        y_chunk = y_train_shuffle[index_start: index_start+chunksize]\n",
    "        yield X_chunk_vec, y_chunk\n",
    "        index_start += chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "34130860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_history = {}\n",
    "\n",
    "for loss in [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\", \"perceptron\"]:\n",
    "    model_sgd = SGDClassifier(loss=loss,\n",
    "                              penalty=None,\n",
    "                              n_jobs=-1, \n",
    "                              random_state=1,\n",
    "                              verbose=0)\n",
    "    iterator = chunk_iterator(chunksize, X_train_shuffle.shape[0])\n",
    "    for X_chunk, y_chunk in iterator:\n",
    "        model_sgd.partial_fit(X_chunk, y_chunk, classes=classes)\n",
    "        \n",
    "    loss_history[loss] = np.mean([model_sgd.score(X_val_vec, y_val), model_sgd.score(X_test_vec, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ea2accca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hinge': 0.8674,\n",
       " 'log_loss': 0.8605,\n",
       " 'modified_huber': 0.8542,\n",
       " 'squared_hinge': 0.8173,\n",
       " 'perceptron': 0.8479}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
